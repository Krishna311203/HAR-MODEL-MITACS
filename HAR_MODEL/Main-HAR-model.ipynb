{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_predict\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import pickle\n",
        "# Read the CSV file\n",
        "file_path = '/content/drive/MyDrive/weiss_data/consolidated_wiess_data_final.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Encode the target column if it's categorical\n",
        "target_column = 'ACTIVITY'  # Replace with the name of your target column\n",
        "df[target_column] = df[target_column].astype('category').cat.codes\n",
        "\n",
        "# Drop the 'class' column from the features\n",
        "X = df.drop(columns=[target_column, 'class'])\n",
        "y = df[target_column]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the base models with hyperparameters\n",
        "rf_model = RandomForestClassifier(n_estimators=200, min_samples_split=2, min_samples_leaf=1, max_depth=20, bootstrap=False, random_state=42)\n",
        "svm_model = SVC(C=100, gamma='auto', kernel='rbf', probability=True, random_state=42)\n",
        "\n",
        "# Get out-of-fold predictions for training the meta-model\n",
        "rf_oof_preds = cross_val_predict(rf_model, X_train, y_train, cv=5, method='predict_proba')\n",
        "svm_oof_preds = cross_val_predict(svm_model, X_train, y_train, cv=5, method='predict_proba')\n",
        "\n",
        "# Stack the predictions as new features\n",
        "stacked_features = np.hstack((rf_oof_preds, svm_oof_preds))\n",
        "\n",
        "# Train the meta-model (Logistic Regression)\n",
        "meta_model = LogisticRegression(random_state=42)\n",
        "meta_model.fit(stacked_features, y_train)\n",
        "\n",
        "# Generate test set predictions using the base models\n",
        "rf_test_preds = rf_model.fit(X_train, y_train).predict_proba(X_test)\n",
        "svm_test_preds = svm_model.fit(X_train, y_train).predict_proba(X_test)\n",
        "\n",
        "# Stack the test set predictions\n",
        "stacked_test_features = np.hstack((rf_test_preds, svm_test_preds))\n",
        "\n",
        "# Predict using the meta-model\n",
        "final_predictions = meta_model.predict(stacked_test_features)\n",
        "\n",
        "# Evaluate the stacking ensemble\n",
        "accuracy = accuracy_score(y_test, final_predictions)\n",
        "conf_matrix = confusion_matrix(y_test, final_predictions)\n",
        "model_filename = '/content/drive/MyDrive/trained_model.pkl'\n",
        "with open(model_filename, 'wb') as model_file:\n",
        "    pickle.dump(meta_model, model_file)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Confusion Matrix:\\n{conf_matrix}')"
      ],
      "metadata": {
        "id": "Lt5jQNQ0AIDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a62913-b387-4790-c4ff-28a648cb0196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8649464726873456\n",
            "Confusion Matrix:\n",
            "[[164   0  17   1   0   0   0   0   0   0   0   0   5   0   0   1   1   3]\n",
            " [  0 180   0   0   0   0   0   0   0   1   0   0   0   1   1   0   0   0]\n",
            " [  7   2 160   0   0   0   0   2   0   0   0   1  10   3   0   0   0   6]\n",
            " [  0   0   1 176   4   9   0   0   3   6   1   3   0   0   0   6   0   1]\n",
            " [  0   0   0   2 187   0   0   3   6   0   1   3   1   3   1   2   0   2]\n",
            " [  0   0   0   4   1 191   0   0   4   0   1   1   0   0   0   9   0   0]\n",
            " [  0   0   1   0   1   0 167   1   5   0   0   1   0   0   0   0   1   1]\n",
            " [  0   0   0   0   2   0   4 153   5  19   6   7   0   0   0   2   0   1]\n",
            " [  0   0   0   1   1   1   1   7 159  12   7  23   0   0   0   0   0   2]\n",
            " [  0   0   0   2   1   2   0   4   7 163   0   7   0   0   0   2   0   1]\n",
            " [  0   0   1   4   1   0   0   0   2   7 168  24   0   0   0   0   0   1]\n",
            " [  0   0   0   4   1   1   0  11  25  14  15 152   0   0   1   2   0   0]\n",
            " [  3   0   7   0   0   0   2   1   1   0   0   1 178   7   0   0   0   0]\n",
            " [  0   3   1   0   0   0   0   0   0   0   1   0   3 185   3   0   0   2]\n",
            " [  2   1   0   0   0   0   0   0   0   1   0   0   1   4 209   0   0   2]\n",
            " [  0   0   0   3   1   2   0   3   2   3   1   2   1   1   2 187   2   2]\n",
            " [  0   0   0   2   0   0   1   1   0   1   1   0   0   1   1   0 172   0]\n",
            " [  0   0   2   0   2   0   1   5   2   0   0   3   1   1   2   0   2 200]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/ACC.csv', skiprows=1)  # Skip the first row with headers\n",
        "\n",
        "# Step 2: Parse the starting timestamp from the first line\n",
        "with open('/content/drive/MyDrive/ACC.csv', 'r') as f:\n",
        "    first_line = f.readline().strip().split(',')\n",
        "    starting_timestamp = float(first_line[0])  # Assuming the timestamp is in the first column\n",
        "\n",
        "# Step 3: Calculate timestamps\n",
        "sampling_frequency = 32  # Hz\n",
        "resolution = 0.015  # seconds\n",
        "\n",
        "# Calculate timestamps\n",
        "timestamps = [starting_timestamp + i / sampling_frequency for i in range(len(df))]\n",
        "df['Timestamp'] = timestamps\n",
        "\n",
        "# Step 4: Rename columns X0, X1, X2 to X, Y0, Y1, Y2 to Y, Z0, Z1, Z2 to Z\n",
        "df = df.rename(columns={\n",
        "    df.columns[0]: 'X',\n",
        "    df.columns[1]: 'Y',\n",
        "    df.columns[2]: 'Z'\n",
        "})\n",
        "\n",
        "# Step 5: Save the updated DataFrame to a new CSV file\n",
        "df.to_csv('/content/drive/MyDrive/ACC_with_time_and_renamed.csv', index=False)\n"
      ],
      "metadata": {
        "id": "rizI7azsKvXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read the CSV file\n",
        "file_path = '/content/drive/MyDrive/angel_data/ACC_with_time_and_renamed.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define the window size and sampling frequency\n",
        "window_size = 10  # seconds\n",
        "sampling_frequency = 32  # Hz\n",
        "window_length = window_size * sampling_frequency\n",
        "\n",
        "# Function to calculate features for each window\n",
        "def calculate_features(window):\n",
        "    features = {}\n",
        "\n",
        "    # Calculate binned distribution\n",
        "    for axis in ['X', 'Y', 'Z']:\n",
        "        min_val = window[axis].min()\n",
        "        max_val = window[axis].max()\n",
        "        bins = np.linspace(min_val, max_val, num=11)  # 10 bins\n",
        "        binned_counts, _ = np.histogram(window[axis], bins=bins)\n",
        "        binned_distribution = binned_counts / len(window)\n",
        "        for i in range(10):\n",
        "            features[f'{axis}{i}'] = binned_distribution[i]\n",
        "\n",
        "    # Average\n",
        "    features['XAVG'] = window['X'].mean()\n",
        "    features['YAVG'] = window['Y'].mean()\n",
        "    features['ZAVG'] = window['Z'].mean()\n",
        "\n",
        "    # Peak detection\n",
        "    features['XPEAK'] = (window['X'].diff().abs() > 0.015).sum()  # Replace with actual peak detection logic\n",
        "    features['YPEAK'] = (window['Y'].diff().abs() > 0.015).sum()  # Replace with actual peak detection logic\n",
        "    features['ZPEAK'] = (window['Z'].diff().abs() > 0.015).sum()  # Replace with actual peak detection logic\n",
        "\n",
        "    # Absolute deviation\n",
        "    features['XABSOLDEV'] = np.abs(window['X'] - features['XAVG']).mean()\n",
        "    features['YABSOLDEV'] = np.abs(window['Y'] - features['YAVG']).mean()\n",
        "    features['ZABSOLDEV'] = np.abs(window['Z'] - features['ZAVG']).mean()\n",
        "\n",
        "    # Standard deviation\n",
        "    features['XSTANDDEV'] = window['X'].std()\n",
        "    features['YSTANDDEV'] = window['Y'].std()\n",
        "    features['ZSTANDDEV'] = window['Z'].std()\n",
        "\n",
        "    # Resultant\n",
        "    resultant = np.sqrt(window['X']**2 + window['Y']**2 + window['Z']**2)\n",
        "    features['RESULTANT'] = resultant.mean()\n",
        "\n",
        "    return features\n",
        "\n",
        "# Process each window and collect features\n",
        "features_list = []\n",
        "for start in range(0, len(df), window_length):\n",
        "    window = df.iloc[start:start + window_length]\n",
        "    if len(window) == window_length:\n",
        "        features = calculate_features(window)\n",
        "        features_list.append(features)\n",
        "\n",
        "# Create a DataFrame from the features\n",
        "features_df = pd.DataFrame(features_list)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/drive/MyDrive/output_with_features.csv'\n",
        "features_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Feature extraction completed. The output is saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "id": "NgToOz6y_F3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a80826-0776-49bf-9659-9ea225730567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature extraction completed. The output is saved to /content/drive/MyDrive/output_with_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to filter predictions to only valid classes\n",
        "def filter_valid_classes(probs, valid_classes):\n",
        "    # Select columns corresponding to valid classes\n",
        "    valid_probs = probs[:, valid_classes]\n",
        "    # Find the index of the class with the highest probability\n",
        "    valid_class_index = np.argmax(valid_probs, axis=1)\n",
        "    # Map back to original class labels\n",
        "    return np.array(valid_classes)[valid_class_index]\n",
        "\n",
        "# Function to make predictions on new data\n",
        "def predict_new_data(new_data_path):\n",
        "    # Read and preprocess the new data\n",
        "    new_df = pd.read_csv(new_data_path)\n",
        "    # new_X = new_df.drop(columns=['class'])  # Adjust column drop if necessary\n",
        "    new_X = scaler.transform(new_df)\n",
        "\n",
        "    # Generate prediction probabilities using base models\n",
        "    rf_new_preds = rf_model.predict_proba(new_X)\n",
        "    svm_new_preds = svm_model.predict_proba(new_X)\n",
        "\n",
        "    # Stack the new prediction probabilities\n",
        "    stacked_new_features = np.hstack((rf_new_preds, svm_new_preds))\n",
        "\n",
        "    # Predict using the meta-model\n",
        "    new_predictions_proba = meta_model.predict_proba(stacked_new_features)\n",
        "\n",
        "    # Filter the predictions to only include valid classes\n",
        "    valid_classes = [0, 1, 3, 4]\n",
        "    filtered_predictions = filter_valid_classes(new_predictions_proba, valid_classes)\n",
        "\n",
        "    return filtered_predictions\n",
        "\n",
        "# Example usage\n",
        "new_data_path = '/content/drive/MyDrive/output_with_features.csv'\n",
        "new_predictions = predict_new_data(new_data_path)\n",
        "print(new_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S_qtsSW9ULu",
        "outputId": "406790cd-434a-457c-c77a-acc79334d5de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standing data\n"
      ],
      "metadata": {
        "id": "W4BQd12JLFlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/angel_data/ACC_standing.csv', skiprows=1)  # Skip the first row with headers\n",
        "\n",
        "# Step 2: Parse the starting timestamp from the first line\n",
        "with open('/content/drive/MyDrive/angel_data/ACC_standing.csv', 'r') as f:\n",
        "    first_line = f.readline().strip().split(',')\n",
        "    starting_timestamp = float(first_line[0])  # Assuming the timestamp is in the first column\n",
        "\n",
        "# Step 3: Calculate timestamps\n",
        "sampling_frequency = 32  # Hz\n",
        "resolution = 0.015  # seconds\n",
        "\n",
        "# Calculate timestamps\n",
        "timestamps = [starting_timestamp + i / sampling_frequency for i in range(len(df))]\n",
        "df['Timestamp'] = timestamps\n",
        "\n",
        "# Step 4: Rename columns X0, X1, X2 to X, Y0, Y1, Y2 to Y, Z0, Z1, Z2 to Z\n",
        "df = df.rename(columns={\n",
        "    df.columns[0]: 'X',\n",
        "    df.columns[1]: 'Y',\n",
        "    df.columns[2]: 'Z'\n",
        "})\n",
        "\n",
        "# Step 5: Save the updated DataFrame to a new CSV file\n",
        "df.to_csv('/content/drive/MyDrive/angel_data/ACC_standing_with_time_and_renamed.csv', index=False)\n"
      ],
      "metadata": {
        "id": "VAd66ZGC_Tkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read the CSV file\n",
        "file_path = '/content/drive/MyDrive/angel_data/ACC_standing_with_time_and_renamed.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define the window size and sampling frequency\n",
        "window_size = 10  # seconds\n",
        "sampling_frequency = 32  # Hz\n",
        "window_length = window_size * sampling_frequency\n",
        "\n",
        "# Function to calculate features for each window\n",
        "def calculate_features(window):\n",
        "    features = {}\n",
        "\n",
        "    # Calculate binned distribution\n",
        "    for axis in ['X', 'Y', 'Z']:\n",
        "        min_val = window[axis].min()\n",
        "        max_val = window[axis].max()\n",
        "        bins = np.linspace(min_val, max_val, num=11)  # 10 bins\n",
        "        binned_counts, _ = np.histogram(window[axis], bins=bins)\n",
        "        binned_distribution = binned_counts / len(window)\n",
        "        for i in range(10):\n",
        "            features[f'{axis}{i}'] = binned_distribution[i]\n",
        "\n",
        "    # Average\n",
        "    features['XAVG'] = window['X'].mean()\n",
        "    features['YAVG'] = window['Y'].mean()\n",
        "    features['ZAVG'] = window['Z'].mean()\n",
        "\n",
        "    # Peak detection\n",
        "    features['XPEAK'] = (window['X'].diff().abs() > 0.015).sum()  # Replace with actual peak detection logic\n",
        "    features['YPEAK'] = (window['Y'].diff().abs() > 0.015).sum()  # Replace with actual peak detection logic\n",
        "    features['ZPEAK'] = (window['Z'].diff().abs() > 0.015).sum()  # Replace with actual peak detection logic\n",
        "\n",
        "    # Absolute deviation\n",
        "    features['XABSOLDEV'] = np.abs(window['X'] - features['XAVG']).mean()\n",
        "    features['YABSOLDEV'] = np.abs(window['Y'] - features['YAVG']).mean()\n",
        "    features['ZABSOLDEV'] = np.abs(window['Z'] - features['ZAVG']).mean()\n",
        "\n",
        "    # Standard deviation\n",
        "    features['XSTANDDEV'] = window['X'].std()\n",
        "    features['YSTANDDEV'] = window['Y'].std()\n",
        "    features['ZSTANDDEV'] = window['Z'].std()\n",
        "\n",
        "    # Resultant\n",
        "    resultant = np.sqrt(window['X']**2 + window['Y']**2 + window['Z']**2)\n",
        "    features['RESULTANT'] = resultant.mean()\n",
        "\n",
        "    return features\n",
        "\n",
        "# Process each window and collect features\n",
        "features_list = []\n",
        "for start in range(0, len(df), window_length):\n",
        "    window = df.iloc[start:start + window_length]\n",
        "    if len(window) == window_length:\n",
        "        features = calculate_features(window)\n",
        "        features_list.append(features)\n",
        "\n",
        "# Create a DataFrame from the features\n",
        "features_df = pd.DataFrame(features_list)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/drive/MyDrive/output_standing_with_features.csv'\n",
        "features_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Feature extraction completed. The output is saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BgKOuzeLfCD",
        "outputId": "eeb2a4c1-c94a-47c2-f73a-f57c4ba47114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature extraction completed. The output is saved to /content/drive/MyDrive/output_standing_with_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to filter predictions to only valid classes\n",
        "def filter_valid_classes(probs, valid_classes):\n",
        "    # Select columns corresponding to valid classes\n",
        "    valid_probs = probs[:, valid_classes]\n",
        "    # Find the index of the class with the highest probability\n",
        "    valid_class_index = np.argmax(valid_probs, axis=1)\n",
        "    # Map back to original class labels\n",
        "    return np.array(valid_classes)[valid_class_index]\n",
        "\n",
        "# Function to make predictions on new data\n",
        "def predict_new_data(new_data_path):\n",
        "    # Read and preprocess the new data\n",
        "    new_df = pd.read_csv(new_data_path)\n",
        "    # new_X = new_df.drop(columns=['class'])  # Adjust column drop if necessary\n",
        "    new_X = scaler.transform(new_df)\n",
        "\n",
        "    # Generate prediction probabilities using base models\n",
        "    rf_new_preds = rf_model.predict_proba(new_X)\n",
        "    svm_new_preds = svm_model.predict_proba(new_X)\n",
        "\n",
        "    # Stack the new prediction probabilities\n",
        "    stacked_new_features = np.hstack((rf_new_preds, svm_new_preds))\n",
        "\n",
        "    # Predict using the meta-model\n",
        "    new_predictions_proba = meta_model.predict_proba(stacked_new_features)\n",
        "\n",
        "    # Filter the predictions to only include valid classes\n",
        "    valid_classes = [0, 1, 3, 4]\n",
        "    filtered_predictions = filter_valid_classes(new_predictions_proba, valid_classes)\n",
        "\n",
        "    return filtered_predictions\n",
        "\n",
        "# Example usage\n",
        "new_data_path = '/content/drive/MyDrive/output_standing_with_features.csv'\n",
        "new_predictions = predict_new_data(new_data_path)\n",
        "print(new_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXyWtkn1L92v",
        "outputId": "c41d7445-94d0-4084-8c3e-cc0e6c90b08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 3 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the base models\n",
        "with open('/content/drive/MyDrive/rf_model.pkl', 'wb') as f:\n",
        "    pickle.dump(rf_model, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/svm_model.pkl', 'wb') as f:\n",
        "    pickle.dump(svm_model, f)\n",
        "\n",
        "# Save the meta-model\n",
        "with open('/content/drive/MyDrive/meta_model.pkl', 'wb') as f:\n",
        "    pickle.dump(meta_model, f)\n",
        "\n",
        "# Save the scaler\n",
        "with open('/content/drive/MyDrive/scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n"
      ],
      "metadata": {
        "id": "DGTrsCZTMG8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WJIncCcJDHVk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}